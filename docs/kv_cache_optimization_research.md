# whisper.cpp KV Cache ä¼˜åŒ–ä¸å‹ç¼©ç ”ç©¶æŠ¥å‘Š

## ç ”ç©¶èƒŒæ™¯ä¸ç›®æ ‡

æœ¬æ–‡æ¡£é’ˆå¯¹åŸºäº `ggml` åº“çš„ `whisper.cpp` é¡¹ç›®ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æ KV Cacheï¼ˆé”®å€¼ç¼“å­˜ï¼‰çš„å®ç°ç°çŠ¶ã€ç†è®ºç“¶é¢ˆï¼Œå¹¶æå‡ºå…·æœ‰å·¥ç¨‹å¯è¡Œæ€§çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶æœåŠ¡äºç¡•å£«è®ºæ–‡ã€Šé¢å‘ç«¯ä¾§è®¾å¤‡çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹è½»é‡åŒ–ä¸åŠ é€Ÿæ–¹æ³•ç ”ç©¶ã€‹ã€‚

---

## ç¬¬ä¸€é˜¶æ®µï¼šç°çŠ¶åˆ†æä¸ç†è®ºç“¶é¢ˆè¯Šæ–­ (Diagnosis & Theory)

### 1.1 æºç é€»è¾‘å®šä½

#### 1.1.1 KV Cache æ•°æ®ç»“æ„å®šä¹‰

åœ¨ `whisper.cpp` æºç ä¸­ï¼ŒKV Cache çš„æ ¸å¿ƒæ•°æ®ç»“æ„å®šä¹‰å¦‚ä¸‹ï¼š

```cpp
// æ–‡ä»¶ä½ç½®: src/whisper.cpp

struct whisper_kv_cell {
    whisper_pos pos = -1;                    // ç¼“å­˜ä½ç½®ç´¢å¼•
    std::set<whisper_seq_id> seq_id;         // åºåˆ—æ ‡è¯†ç¬¦é›†åˆ

    bool has_seq_id(const whisper_seq_id & id) const {
        return seq_id.find(id) != seq_id.end();
    }
};

struct whisper_kv_cache {
    uint32_t head = 0;                       // å½“å‰å†™å…¥å¤´æŒ‡é’ˆ
    uint32_t size = 0;                       // ç¼“å­˜å®¹é‡ï¼ˆn_ctxï¼‰

    uint32_t n = 0;                          // æ¯æ¬¡å›¾æ„å»ºå‰è®¡ç®—çš„æœ‰æ•ˆé•¿åº¦

    std::vector<whisper_kv_cell> cells;      // ç¼“å­˜æ§½ä½å…ƒæ•°æ®

    struct ggml_tensor * k;                  // Key å¼ é‡ç¼“å­˜
    struct ggml_tensor * v;                  // Value å¼ é‡ç¼“å­˜

    ggml_backend_buffer_t buffer = nullptr;  // åç«¯å†…å­˜ç¼“å†²åŒº

    std::vector<uint8_t> ctx_buf;            // ggml ä¸Šä¸‹æ–‡ç¼“å†²åŒº
};
```

åœ¨ `whisper_state` ç»“æ„ä½“ä¸­ï¼Œå­˜åœ¨ä¸‰ç§ KV Cache å®ä¾‹ï¼š

```cpp
struct whisper_state {
    // ...
    whisper_kv_cache kv_self;    // Decoder Self-Attention çš„ç»Ÿä¸€ KV Cache
    whisper_kv_cache kv_cross;   // Cross-Attention çš„ KV Cacheï¼ˆEncoder è¾“å‡ºï¼‰
    whisper_kv_cache kv_pad;     // Flash Attention çš„å¡«å……ç¼“å†²åŒº
    // ...
};
```

#### 1.1.2 KV Cache å†…å­˜åˆ†é…

KV Cache çš„åˆå§‹åŒ–é€šè¿‡ `whisper_kv_cache_init` å‡½æ•°å®Œæˆï¼š

```cpp
static bool whisper_kv_cache_init(
             struct whisper_kv_cache & cache,
                      ggml_backend_t   backend,
                           ggml_type   wtype,      // æƒé‡ç±»å‹ (FP16/FP32)
                             int64_t   n_text_state,
                             int64_t   n_text_layer,
                                 int   n_ctx) {
    const int64_t n_mem      = n_text_layer * n_ctx;
    const int64_t n_elements = n_text_state * n_mem;

    // åˆ†é… K å’Œ V å¼ é‡
    cache.k = ggml_new_tensor_1d(ctx, wtype, n_elements);
    cache.v = ggml_new_tensor_1d(ctx, wtype, n_elements);

    // åœ¨åç«¯ï¼ˆCPU/GPUï¼‰åˆ†é…å®é™…å†…å­˜
    cache.buffer = ggml_backend_alloc_ctx_tensors(ctx, backend);
    // ...
}
```

**å…³é”®å‚æ•°è§£æï¼š**
- `n_text_state`: éšè—å±‚ç»´åº¦ $d_{model}$ï¼ˆå¦‚ Whisper Base ä¸º 512ï¼ŒLarge ä¸º 1280ï¼‰
- `n_text_layer`: Decoder å±‚æ•° $L$ï¼ˆå¦‚ Whisper Base ä¸º 6 å±‚ï¼ŒLarge ä¸º 32 å±‚ï¼‰
- `n_ctx`: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé»˜è®¤ 448 ä¸ª tokenï¼‰
- `wtype`: æ•°æ®ç±»å‹ï¼Œé€šå¸¸ä¸º `GGML_TYPE_F16`

#### 1.1.3 KV Cache æ›´æ–°æœºåˆ¶

åœ¨ Decoder çš„ Self-Attention è®¡ç®—è¿‡ç¨‹ä¸­ï¼ŒKV Cache çš„æ›´æ–°é€»è¾‘ä½äº `whisper_build_graph_decoder` å‡½æ•°ï¼š

```cpp
// è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„ K å’Œ V
struct ggml_tensor * Kcur = ggml_mul_mat(ctx0, layer.attn_k_w, cur);
struct ggml_tensor * Vcur = ggml_mul_mat(ctx0, layer.attn_v_w, cur);
Vcur = ggml_add(ctx0, Vcur, layer.attn_v_b);

// å°† Kcur å’Œ Vcur å†™å…¥ KV Cache
struct ggml_tensor * k = ggml_view_1d(ctx0, kv_self.k, n_tokens * n_state,
        (ggml_element_size(kv_self.k) * n_state) * (il * n_ctx + kv_head));

struct ggml_tensor * v = ggml_view_2d(ctx0, kv_self.v, n_tokens, n_state,
        (n_ctx) * ggml_element_size(kv_self.v),
        (il * n_ctx) * ggml_element_size(kv_self.v) * n_state + kv_head * ggml_element_size(kv_self.v));

// ä½¿ç”¨ ggml_cpy å°†è®¡ç®—ç»“æœå¤åˆ¶åˆ°ç¼“å­˜
ggml_build_forward_expand(gf, ggml_cpy(ctx0, Kcur, k));
ggml_build_forward_expand(gf, ggml_cpy(ctx0, Vcur, v));
```

### 1.2 ç“¶é¢ˆç†è®ºåˆ†æ

#### 1.2.1 ç©ºé—´å¤æ‚åº¦åˆ†æ

è®¾ Decoder æœ‰ $L$ å±‚ï¼Œæ¯å±‚æœ‰ $h$ ä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ¯ä¸ªå¤´çš„ç»´åº¦ä¸º $d_k = d_v = d_{model}/h$ã€‚å¯¹äºåºåˆ—é•¿åº¦ä¸º $n$ çš„è¾“å…¥ï¼š

$$
\text{KV Cache ç©ºé—´} = 2 \times L \times n \times d_{model} \times \text{sizeof}(\text{dtype})
$$

ä»¥ Whisper Large (V3) ä¸ºä¾‹ï¼š
- $L = 32$, $d_{model} = 1280$, $n_{ctx} = 448$, `dtype = FP16 (2 bytes)`

$$
\text{Memory} = 2 \times 32 \times 448 \times 1280 \times 2 = 73,400,320 \text{ bytes} \approx 70 \text{ MB}
$$

å¯¹äºé•¿éŸ³é¢‘æ¨ç†ï¼ˆå¤šä¸ª 30 ç§’ç‰‡æ®µè¿ç»­å¤„ç†ï¼‰ï¼ŒKV Cache æˆä¸ºä¸»è¦çš„å†…å­˜ç“¶é¢ˆã€‚

#### 1.2.2 æ—¶é—´å¤æ‚åº¦åˆ†æ

åœ¨æ ‡å‡† Self-Attention è®¡ç®—ä¸­ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

æ¶‰åŠ KV Cache çš„æ ¸å¿ƒæ“ä½œå¤æ‚åº¦å¦‚ä¸‹ï¼š

| æ“ä½œ | æ—¶é—´å¤æ‚åº¦ | è¯´æ˜ |
|------|-----------|------|
| $Q \times K^T$ | $O(n \cdot L \cdot d_{model})$ | çŸ©é˜µä¹˜æ³•ï¼Œn ä¸ºå½“å‰åºåˆ—é•¿åº¦ |
| $\text{softmax}(QK^T) \times V$ | $O(n \cdot L \cdot d_{model})$ | çŸ©é˜µä¹˜æ³• |
| KV Cache è¯»å– | $O(L \cdot n \cdot d_{model})$ | å†…å­˜å¸¦å®½å—é™ |
| KV Cache å†™å…¥ | $O(L \cdot d_{model})$ | æ¯æ­¥å†™å…¥ 1 ä¸ª token |

**å…³é”®ç“¶é¢ˆï¼š** éšç€è§£ç æ­¥æ•° $t$ å¢åŠ ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦è¯»å–å®Œæ•´çš„ KV Cache è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œå¯¼è‡´ï¼š
1. **å†…å­˜å¸¦å®½ç“¶é¢ˆ**ï¼š$O(t \cdot L \cdot d_{model})$ çš„æ•°æ®è¯»å–é‡
2. **è®¡ç®—é‡çº¿æ€§å¢é•¿**ï¼šæ³¨æ„åŠ›è®¡ç®—çš„ FLOPs ä¸ $t$ æˆæ­£æ¯”

#### 1.2.3 å†…å­˜å¸¦å®½åˆ†æ

ç°ä»£ç«¯ä¾§è®¾å¤‡çš„å†…å­˜å¸¦å®½æ˜¯ä¸»è¦ç“¶é¢ˆã€‚ä»¥å…¸å‹ç§»åŠ¨è®¾å¤‡ä¸ºä¾‹ï¼š

| è®¾å¤‡ç±»å‹ | å†…å­˜å¸¦å®½ | Whisper Large KV è¯»å–æ—¶é—´ (448 tokens) |
|----------|---------|----------------------------------------|
| æ ‘è“æ´¾ 4B | ~4 GB/s | ~17.5 ms |
| é«˜ç«¯æ‰‹æœº (LPDDR5) | ~50 GB/s | ~1.4 ms |
| Nvidia Jetson Nano | ~25.6 GB/s | ~2.7 ms |

**ç»“è®ºï¼š** åœ¨ä½å¸¦å®½è®¾å¤‡ä¸Šï¼ŒKV Cache çš„è¯»å–å»¶è¿Ÿæˆä¸ºæ¨ç†é€Ÿåº¦çš„å…³é”®ç“¶é¢ˆã€‚

### 1.3 ç°æœ‰å®ç°çš„ç¼ºé™·åˆ†æ

#### 1.3.1 FP16 å­˜å‚¨çš„ç²¾åº¦å†—ä½™

å½“å‰ `whisper.cpp` é»˜è®¤ä½¿ç”¨ FP16 å­˜å‚¨ KV Cacheã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼š
- Attention æœºåˆ¶å¯¹ K/V å€¼çš„ç²¾åº¦æ•æ„Ÿåº¦ä½äºæ¨¡å‹æƒé‡
- K/V å€¼çš„æ•°å€¼èŒƒå›´é€šå¸¸é›†ä¸­åœ¨ $[-3, 3]$ åŒºé—´
- 8-bit é‡åŒ–ï¼ˆINT8ï¼‰åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸ä¼šæ˜¾è‘—å½±å“æœ€ç»ˆè¯†åˆ«ç²¾åº¦

#### 1.3.2 é™æ€å†…å­˜åˆ†é…

å½“å‰å®ç°é¢„åˆ†é…å®Œæ•´çš„ `n_ctx * n_layer * n_state` å¤§å°çš„ç¼“å­˜ï¼Œå³ä½¿å®é™…åºåˆ—é•¿åº¦è¿œå°äºæœ€å¤§å€¼ï¼Œä¹Ÿä¼šå ç”¨å…¨éƒ¨å†…å­˜ã€‚

#### 1.3.3 ç¼ºä¹ç¼“å­˜å¤ç”¨æœºåˆ¶

å¯¹äºé•¿éŸ³é¢‘çš„åˆ†æ®µå¤„ç†ï¼Œæ¯ä¸ª 30 ç§’ç‰‡æ®µéƒ½éœ€è¦é‡æ–°åˆå§‹åŒ– KV Cacheï¼Œç¼ºä¹è·¨ç‰‡æ®µçš„ç¼“å­˜å¤ç”¨ä¼˜åŒ–ã€‚

---

## ç¬¬äºŒé˜¶æ®µï¼šä¼˜åŒ–ç­–ç•¥è®¾è®¡ (Methodology)

### 2.1 æ–¹æ¡ˆ Aï¼šKV Cache ä½æ¯”ç‰¹é‡åŒ– (é¦–é€‰æ–¹æ¡ˆ)

#### 2.1.1 é‡åŒ–æ–¹æ¡ˆè®¾è®¡

å°† KV Cache ä» FP16 é™çº§ä¸º INT8 (Q8_0 æ ¼å¼)ï¼š

$$
\text{é‡åŒ–}: x_{int8} = \text{round}\left(\frac{x_{fp16}}{\text{scale}}\right), \quad \text{scale} = \frac{\max(|x|)}{127}
$$

$$
\text{åé‡åŒ–}: x_{fp16} = x_{int8} \times \text{scale}
$$

**é¢„æœŸæ”¶ç›Šï¼š**
- å†…å­˜å ç”¨é™ä½ 50%ï¼ˆFP16 â†’ INT8ï¼‰
- å†…å­˜å¸¦å®½éœ€æ±‚é™ä½ 50%
- æ¨ç†å»¶è¿Ÿé¢„æœŸé™ä½ 30-40%

#### 2.1.2 ggml Q8_0 æ ¼å¼è¯´æ˜

`GGML_TYPE_Q8_0` çš„æ•°æ®å¸ƒå±€ï¼ˆblock size = 32ï¼‰ï¼š

```c
typedef struct {
    ggml_fp16_t d;       // é‡åŒ– scale (delta)
    int8_t  qs[32];      // 32 ä¸ªé‡åŒ–å€¼
} block_q8_0;
```

æ¯ 32 ä¸ª INT8 å€¼å…±äº«ä¸€ä¸ª FP16 çš„ scale å› å­ï¼Œæœ‰æ•ˆæ¯”ç‰¹ç‡ä¸ºï¼š
$$
\text{bits per value} = 8 + \frac{16}{32} = 8.5 \text{ bits}
$$

#### 2.1.3 éœ€è¦ä¿®æ”¹çš„ç®—å­

| ç®—å­ | å½“å‰çŠ¶æ€ | ä¿®æ”¹è¯´æ˜ |
|------|----------|----------|
| `ggml_cpy` | âœ… å·²æ”¯æŒ F32â†’Q8_0 | å¯ç›´æ¥ç”¨äº KV å†™å…¥æ—¶é‡åŒ– |
| `ggml_mul_mat` | âœ… å·²æ”¯æŒ Q8_0Ã—F32/F16 | å¯ç›´æ¥ç”¨äº Attention è®¡ç®— |
| Flash Attention | âš ï¸ éƒ¨åˆ†æ”¯æŒ | éœ€éªŒè¯ `ggml_flash_attn_ext` çš„é‡åŒ–æ”¯æŒ |

### 2.2 æ–¹æ¡ˆ Bï¼šæ»‘åŠ¨çª—å£æ³¨æ„åŠ› (å¤‡é€‰æ–¹æ¡ˆ)

#### 2.2.1 è®¾è®¡æ€è·¯

é™åˆ¶ Self-Attention çš„æœ‰æ•ˆçª—å£å¤§å°ä¸º $w < n_{ctx}$ï¼š

$$
\text{Attention}(Q, K_w, V_w) = \text{softmax}\left(\frac{QK_w^T}{\sqrt{d_k}}\right) V_w
$$

å…¶ä¸­ $K_w, V_w$ ä»…åŒ…å«æœ€è¿‘ $w$ ä¸ª token çš„ç¼“å­˜ã€‚

#### 2.2.2 å®ç°å¤æ‚åº¦

- éœ€è¦ä¿®æ”¹ `whisper_kv_cache_find_slot` çš„æ§½ä½åˆ†é…é€»è¾‘
- éœ€è¦å®ç° Circular Buffer æœºåˆ¶
- å¯èƒ½å½±å“é•¿è·ç¦»ä¾èµ–çš„å»ºæ¨¡èƒ½åŠ›

**ç»“è®ºï¼š** æ–¹æ¡ˆ B çš„å®ç°å¤æ‚åº¦è¾ƒé«˜ï¼Œä¸”å¯èƒ½å½±å“è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œå»ºè®®ä¼˜å…ˆå®æ–½æ–¹æ¡ˆ Aã€‚

---

## ç¬¬ä¸‰é˜¶æ®µï¼šä»£ç å®ç°æŒ‡å¯¼ (Implementation Guide)

### 3.1 æ•°æ®ç»“æ„ä¿®æ”¹

#### 3.1.1 æ·»åŠ  KV Cache ç±»å‹é…ç½®

åœ¨ `whisper_context_params` ä¸­æ·»åŠ  KV Cache é‡åŒ–é€‰é¡¹ï¼š

```cpp
// æ–‡ä»¶: include/whisper.h

struct whisper_context_params {
    bool  use_gpu;
    bool  flash_attn;
    int   gpu_device;

    // æ–°å¢: KV Cache é‡åŒ–é…ç½®
    bool  kv_cache_quantize;      // æ˜¯å¦å¯ç”¨ KV Cache é‡åŒ–
    // é‡åŒ–ç±»å‹ç”±å†…éƒ¨å›ºå®šä¸º GGML_TYPE_Q8_0

    // ... å…¶ä»–æˆå‘˜
};
```

#### 3.1.2 ä¿®æ”¹ KV Cache åˆå§‹åŒ–

```cpp
// æ–‡ä»¶: src/whisper.cpp

static bool whisper_kv_cache_init(
             struct whisper_kv_cache & cache,
                      ggml_backend_t   backend,
                           ggml_type   wtype,
                             int64_t   n_text_state,
                             int64_t   n_text_layer,
                                 int   n_ctx,
                                bool   quantize = false) {  // æ–°å¢å‚æ•°
    const int64_t n_mem      = n_text_layer * n_ctx;
    const int64_t n_elements = n_text_state * n_mem;

    // æ ¹æ®é‡åŒ–é…ç½®é€‰æ‹©æ•°æ®ç±»å‹
    ggml_type kv_type = quantize ? GGML_TYPE_Q8_0 : wtype;

    cache.k = ggml_new_tensor_1d(ctx, kv_type, n_elements);
    cache.v = ggml_new_tensor_1d(ctx, kv_type, n_elements);

    cache.buffer = ggml_backend_alloc_ctx_tensors(ctx, backend);
    // ...
}
```

### 3.2 å…³é”®å‡½æ•°ä¿®æ”¹

#### 3.2.1 KV Cache å†™å…¥æ—¶çš„é‡åŒ–å¤„ç†

åœ¨ `whisper_build_graph_decoder` ä¸­ï¼Œå°† FP16/FP32 çš„ K/V è®¡ç®—ç»“æœé‡åŒ–åå†™å…¥ç¼“å­˜ï¼š

```cpp
// æ–‡ä»¶: src/whisper.cpp - whisper_build_graph_decoder å‡½æ•°

// store key and value to memory (with optional quantization)
{
    struct ggml_tensor * Vcur = ggml_mul_mat(ctx0, layer.attn_v_w, cur);
    Vcur = ggml_add(ctx0, Vcur, layer.attn_v_b);

    struct ggml_tensor * k;
    struct ggml_tensor * v;

    // åˆ›å»ºæŒ‡å‘ KV Cache å¯¹åº”ä½ç½®çš„è§†å›¾
    k = ggml_view_1d(ctx0, kv_self.k, n_tokens * n_state,
            (ggml_element_size(kv_self.k) * n_state) * (il * n_ctx + kv_head));

    v = ggml_view_1d(ctx0, kv_self.v, n_tokens * n_state,
            (ggml_element_size(kv_self.v) * n_state) * (il * n_ctx + kv_head));

    // ggml_cpy ä¼šè‡ªåŠ¨å¤„ç†ç±»å‹è½¬æ¢ï¼ˆåŒ…æ‹¬é‡åŒ–ï¼‰
    // å½“ k/v çš„ç±»å‹ä¸º Q8_0 æ—¶ï¼Œggml_cpy ä¼šè°ƒç”¨å†…éƒ¨çš„é‡åŒ–å‡½æ•°
    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Kcur, k));
    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Vcur, v));
}
```

**è¯´æ˜ï¼š** `ggml_cpy` ç®—å­å·²åŸç”Ÿæ”¯æŒ `F32/F16 â†’ Q8_0` çš„ç±»å‹è½¬æ¢ï¼Œæ— éœ€é¢å¤–å®ç°é‡åŒ–å‡½æ•°ã€‚

#### 3.2.2 Attention è®¡ç®—æ—¶çš„å¤„ç†

ggml çš„ `ggml_mul_mat` å·²æ”¯æŒ Q8_0 ç±»å‹çš„çŸ©é˜µä¹˜æ³•ï¼š

```cpp
// K * Q è®¡ç®— (K ä¸º Q8_0 ç±»å‹)
struct ggml_tensor * K = ggml_view_3d(ctx0, kv_self.k,
        n_state_head, n_kv, n_head,
        ggml_element_size(kv_self.k) * n_state,
        ggml_element_size(kv_self.k) * n_state_head,
        ggml_element_size(kv_self.k) * n_state * n_ctx * il);

// ggml_mul_mat æ”¯æŒ Q8_0 Ã— F16/F32 çš„æ··åˆç²¾åº¦è®¡ç®—
// å†…éƒ¨ä¼šè‡ªåŠ¨è¿›è¡Œåé‡åŒ–
struct ggml_tensor * KQ = ggml_mul_mat(ctx0, K, Q);
```

**è®¡ç®—æµç¨‹ï¼š**
1. è¯»å– Q8_0 æ ¼å¼çš„ K Cache
2. åœ¨è®¡ç®—å‰è‡ªåŠ¨åé‡åŒ–ä¸º FP32
3. æ‰§è¡ŒçŸ©é˜µä¹˜æ³• $QK^T$
4. ç»“æœä¿æŒ FP32 ç²¾åº¦

### 3.3 å®Œæ•´ä¿®æ”¹ä»£ç ç¤ºä¾‹

ä»¥ä¸‹æ˜¯æ ¸å¿ƒä¿®æ”¹çš„å®Œæ•´ç¤ºä¾‹ï¼š

```cpp
// ========== 1. ä¿®æ”¹ whisper_context_params (include/whisper.h) ==========

struct whisper_context_params {
    bool  use_gpu;
    bool  flash_attn;
    int   gpu_device;

    // KV Cache é‡åŒ–é€‰é¡¹
    bool  kv_cache_q8_0;  // ä½¿ç”¨ Q8_0 æ ¼å¼å­˜å‚¨ KV Cache

    // ... å…¶ä»–æˆå‘˜
};

// ========== 2. ä¿®æ”¹é»˜è®¤å‚æ•° (src/whisper.cpp) ==========

struct whisper_context_params whisper_context_default_params() {
    struct whisper_context_params result = {
        /*.use_gpu             =*/ true,
        /*.flash_attn          =*/ false,
        /*.gpu_device          =*/ 0,
        /*.kv_cache_q8_0       =*/ false,  // é»˜è®¤å…³é—­
        // ...
    };
    return result;
}

// ========== 3. ä¿®æ”¹ KV Cache åˆå§‹åŒ– (src/whisper.cpp) ==========

static bool whisper_kv_cache_init(
             struct whisper_kv_cache & cache,
                      ggml_backend_t   backend,
                           ggml_type   wtype,
                             int64_t   n_text_state,
                             int64_t   n_text_layer,
                                 int   n_ctx,
                                bool   use_q8_0) {
    const int64_t n_mem      = n_text_layer * n_ctx;
    const int64_t n_elements = n_text_state * n_mem;

    cache.ctx_buf.resize(2 * ggml_tensor_overhead());

    struct ggml_init_params params = {
        /*.mem_size   =*/ cache.ctx_buf.size(),
        /*.mem_buffer =*/ cache.ctx_buf.data(),
        /*.no_alloc   =*/ true,
    };

    cache.head = 0;
    cache.size = n_ctx;
    cache.cells.clear();
    cache.cells.resize(n_ctx);

    struct ggml_context * ctx = ggml_init(params);
    if (!ctx) {
        WHISPER_LOG_ERROR("%s: failed to allocate memory for kv cache context\n", __func__);
        return false;
    }

    // æ ¹æ®é…ç½®é€‰æ‹© KV Cache æ•°æ®ç±»å‹
    ggml_type kv_type = use_q8_0 ? GGML_TYPE_Q8_0 : wtype;

    cache.k = ggml_new_tensor_1d(ctx, kv_type, n_elements);
    cache.v = ggml_new_tensor_1d(ctx, kv_type, n_elements);

    cache.buffer = ggml_backend_alloc_ctx_tensors(ctx, backend);
    if (!cache.buffer) {
        WHISPER_LOG_ERROR("%s: failed to allocate memory for kv cache\n", __func__);
        return false;
    }

    // è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
    size_t kv_size = ggml_nbytes(cache.k) + ggml_nbytes(cache.v);
    WHISPER_LOG_INFO("%s: KV cache type: %s, size: %.2f MB\n",
        __func__,
        use_q8_0 ? "Q8_0" : ggml_type_name(wtype),
        kv_size / 1024.0 / 1024.0);

    ggml_backend_buffer_clear(cache.buffer, 0);
    ggml_free(ctx);

    return true;
}

// ========== 4. ä¿®æ”¹è°ƒç”¨ç‚¹ (src/whisper.cpp - whisper_init_state) ==========

struct whisper_state * whisper_init_state(whisper_context * ctx) {
    // ...

    // åˆå§‹åŒ– Self-Attention KV Cache
    if (!whisper_kv_cache_init(
            state->kv_self,
            state->backends[0],
            ctx->itype,
            hparams.n_text_state,
            hparams.n_text_layer,
            hparams.n_text_ctx,
            ctx->params.kv_cache_q8_0)) {  // ä¼ é€’é‡åŒ–é…ç½®
        WHISPER_LOG_ERROR("%s: whisper_kv_cache_init() failed for self-attention cache\n", __func__);
        whisper_free_state(state);
        return nullptr;
    }

    // Cross-Attention KV Cache é€šå¸¸ä¸éœ€è¦é‡åŒ–ï¼ˆä¸€æ¬¡è®¡ç®—å¤šæ¬¡ä½¿ç”¨ï¼‰
    if (!whisper_kv_cache_init(
            state->kv_cross,
            state->backends[0],
            ctx->itype,
            hparams.n_audio_state,
            hparams.n_text_layer,
            hparams.n_audio_ctx,
            false)) {  // Cross-attention ä¸é‡åŒ–
        // ...
    }

    // ...
}
```

### 3.4 éªŒè¯ä¸æµ‹è¯•å»ºè®®

#### 3.4.1 æ­£ç¡®æ€§éªŒè¯

1. **æ•°å€¼ç²¾åº¦æµ‹è¯•**ï¼šæ¯”è¾ƒé‡åŒ–å‰åçš„ KV å€¼è¯¯å·®
   ```cpp
   // æµ‹è¯•ä»£ç ç¤ºä¾‹
   float max_error = 0.0f;
   for (int i = 0; i < n_elements; i++) {
       float original = original_kv[i];
       float quantized = dequantize(quantized_kv[i]);
       max_error = std::max(max_error, std::abs(original - quantized));
   }
   WHISPER_LOG_INFO("KV Cache quantization max error: %f\n", max_error);
   ```

2. **Word Error Rate (WER) æµ‹è¯•**ï¼šåœ¨æ ‡å‡†æ•°æ®é›†ï¼ˆå¦‚ LibriSpeechï¼‰ä¸Šå¯¹æ¯”è¯†åˆ«å‡†ç¡®ç‡

#### 3.4.2 æ€§èƒ½æµ‹è¯•

1. **å†…å­˜å ç”¨æµ‹è¯•**ï¼š
   ```bash
   # ä½¿ç”¨ main ç¤ºä¾‹ç¨‹åº
   ./main -m models/ggml-base.bin -f samples/jfk.wav --kv-cache-q8

   # è§‚å¯Ÿå†…å­˜ä½¿ç”¨
   # macOS: leaks --atExit -- ./main ...
   # Linux: valgrind --tool=massif ./main ...
   ```

2. **æ¨ç†å»¶è¿Ÿæµ‹è¯•**ï¼š
   ```bash
   # ä½¿ç”¨ bench ç¤ºä¾‹
   ./bench -m models/ggml-large-v3.bin -t 4
   ```

### 3.5 æ½œåœ¨é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| Flash Attention ä¸æ”¯æŒ Q8_0 è¾“å…¥ | åœ¨ Flash Attention è·¯å¾„ä¸­ä¿æŒ FP16ï¼Œä»…æ ‡å‡†è·¯å¾„ä½¿ç”¨ Q8_0 |
| GPU åç«¯ä¸æ”¯æŒ Q8_0 cpy | éªŒè¯ CUDA/Metal åç«¯çš„ cpy å®ç°ï¼Œå¿…è¦æ—¶æ·»åŠ  fallback |
| è¯†åˆ«ç²¾åº¦ä¸‹é™ | å¯é€‰æ‹©ä»…å¯¹ V Cache é‡åŒ–ï¼ŒK Cache ä¿æŒ FP16 |

### 3.6 å®ç°æŒ‘æˆ˜ï¼šé‡åŒ–ç±»å‹çš„å—å¯¹é½é—®é¢˜

**é‡è¦å‘ç°ï¼š** åœ¨å®é™…å®ç°è¿‡ç¨‹ä¸­ï¼Œå‘ç°äº†ä¸€ä¸ªå…³é”®çš„æŠ€æœ¯éšœç¢ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ã€‚

#### 3.6.1 é—®é¢˜æè¿°

ggml çš„é‡åŒ–ç±»å‹ï¼ˆå¦‚ Q8_0ï¼‰é‡‡ç”¨å—é‡åŒ–ï¼ˆBlock Quantizationï¼‰ç»“æ„ï¼š

```c
// Q8_0 çš„æ•°æ®å¸ƒå±€ (block size = 32)
typedef struct {
    ggml_fp16_t d;       // é‡åŒ– scale (delta)ï¼Œ2 bytes
    int8_t  qs[32];      // 32 ä¸ªé‡åŒ–å€¼ï¼Œ32 bytes
} block_q8_0;            // æ€»è®¡ 34 bytes per block
```

è¿™æ„å‘³ç€ï¼š
- æ¯ 32 ä¸ªå…ƒç´ å…±äº«ä¸€ä¸ª scale å› å­
- ä¸èƒ½åœ¨ä»»æ„å­—èŠ‚åç§»å¤„åˆ›å»ºè§†å›¾
- `ggml_element_size()` å¯¹é‡åŒ–ç±»å‹è¿”å›çš„æ˜¯é€»è¾‘å…ƒç´ å¤§å°ï¼Œä¸æ˜¯å®é™…å­—èŠ‚å¤§å°

#### 3.6.2 whisper.cpp ä¸­çš„å…¼å®¹æ€§é—®é¢˜

å½“å‰ `whisper_build_graph_decoder` ä¸­ä½¿ç”¨çš„è§†å›¾åˆ›å»ºæ–¹å¼ä¸é‡åŒ–ç±»å‹ä¸å…¼å®¹ï¼š

```cpp
// é—®é¢˜ä»£ç ï¼šä½¿ç”¨ ggml_element_size è®¡ç®—åç§»é‡
k = ggml_view_1d(ctx0, kv_self.k, n_tokens*n_state,
        (ggml_element_size(kv_self.k)*n_state)*(il*n_ctx + kv_head));
```

å¯¹äº Q8_0 ç±»å‹ï¼Œ`ggml_element_size()` è¿”å›çº¦ 1.0625 bytesï¼ˆ34/32ï¼‰ï¼Œä½†å®é™…æ•°æ®æ˜¯ä»¥ 34 å­—èŠ‚çš„å—ä¸ºå•ä½å­˜å‚¨çš„ã€‚è¿™å¯¼è‡´è®¡ç®—çš„åç§»é‡ä¸å¯¹é½åˆ°å—è¾¹ç•Œï¼Œå¼•å‘æ–­è¨€å¤±è´¥ï¼š

```
GGML_ASSERT(view_src == NULL || data_size == 0 || data_size + view_offs <= ggml_nbytes(view_src)) failed
```

#### 3.6.3 æ­£ç¡®çš„å®ç°æ–¹æ¡ˆ

è¦æ­£ç¡®å®ç° KV Cache é‡åŒ–ï¼Œéœ€è¦è¿›è¡Œä»¥ä¸‹ä¿®æ”¹ï¼š

1. **ä½¿ç”¨ `ggml_row_size()` è®¡ç®—å­—èŠ‚åç§»**ï¼š
```cpp
// æ­£ç¡®æ–¹å¼ï¼šä½¿ç”¨ ggml_row_size è®¡ç®—è¡Œçš„å­—èŠ‚å¤§å°
size_t row_bytes = ggml_row_size(kv_self.k->type, n_state);
k = ggml_view_1d(ctx0, kv_self.k, n_tokens*n_state,
        row_bytes * (il*n_ctx + kv_head));
```

2. **ç¡®ä¿ç»´åº¦å¯¹é½åˆ°å—å¤§å°**ï¼š
```cpp
// ç¡®ä¿ n_state æ˜¯ 32 çš„å€æ•°ï¼ˆQ8_0 å—å¤§å°ï¼‰
const int64_t n_state_aligned = GGML_PAD(n_state, 32);
```

3. **ä¿®æ”¹ KV Cache å¼ é‡çš„åˆ›å»ºæ–¹å¼**ï¼š
```cpp
// ä½¿ç”¨ 2D å¼ é‡è€Œé 1Dï¼Œä¾¿äºè¡Œå¯¹é½
cache.k = ggml_new_tensor_2d(ctx, kv_type, n_state_aligned, n_mem);
cache.v = ggml_new_tensor_2d(ctx, kv_type, n_state_aligned, n_mem);
```

#### 3.6.4 æ··åˆç²¾åº¦ç­–ç•¥

ç”¨æˆ·æå‡ºçš„æ··åˆç²¾åº¦ç­–ç•¥æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç ”ç©¶æ–¹å‘ï¼š

1. **K/V åˆ†ç¦»ç²¾åº¦**ï¼š
   - K Cache ä½¿ç”¨æ›´é«˜ç²¾åº¦ï¼ˆFP16ï¼‰ï¼šK ç”¨äºè®¡ç®— attention scoreï¼Œå¯¹ç²¾åº¦æ›´æ•æ„Ÿ
   - V Cache ä½¿ç”¨è¾ƒä½ç²¾åº¦ï¼ˆQ8_0 æˆ– Q4_0ï¼‰ï¼šV ç”¨äºåŠ æƒæ±‚å’Œï¼Œç²¾åº¦è¦æ±‚è¾ƒä½

2. **å±‚çº§å·®å¼‚åŒ–ç²¾åº¦**ï¼š
   - åº•å±‚ï¼ˆé è¿‘è¾“å…¥ï¼‰ï¼šä½¿ç”¨è¾ƒä½ç²¾åº¦
   - é«˜å±‚ï¼ˆé è¿‘è¾“å‡ºï¼‰ï¼šä½¿ç”¨è¾ƒé«˜ç²¾åº¦

3. **æ—¶é—´è¡°å‡ç­–ç•¥**ï¼š
   - è¾ƒæ–°çš„ tokenï¼šä½¿ç”¨è¾ƒé«˜ç²¾åº¦
   - è¾ƒæ—§çš„ tokenï¼šä½¿ç”¨è¾ƒä½ç²¾åº¦ï¼ˆéšæ—¶é—´é€æ­¥é‡åŒ–ï¼‰

**âœ… å·²å®ç°**ï¼šK/V åˆ†ç¦»ç²¾åº¦åŠŸèƒ½å·²æ·»åŠ åˆ° `whisper_context_params` ä¸­ï¼š

```cpp
struct whisper_context_params {
    // ...
    enum ggml_type type_k;  // K cache type (default: F16)
    enum ggml_type type_v;  // V cache type (default: F16)
    // ...
};
```

**ä½¿ç”¨æ–¹æ³•**ï¼š

```cpp
// API ä½¿ç”¨
whisper_context_params cparams = whisper_context_default_params();
cparams.type_k = GGML_TYPE_F16;  // K cache ä½¿ç”¨ FP16
cparams.type_v = GGML_TYPE_F32;  // V cache ä½¿ç”¨ FP32 (æ›´é«˜ç²¾åº¦)
```

```bash
# CLI ä½¿ç”¨
./bin/whisper-cli -m model.bin -f audio.wav --kv-type-k f16 --kv-type-v f32
```

### 3.6.5 é‡åŒ– KV Cache æ€§èƒ½åˆ†æ

**é‡è¦å‘ç°**ï¼šKV Cache é‡åŒ–ï¼ˆå¦‚ Q8_0ï¼‰ç›®å‰ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼ŒåŸå› æ˜¯ ggml çš„ flash attention å®ç°éœ€è¦åœ¨æ¯æ¬¡ attention è®¡ç®—æ—¶å®æ—¶åé‡åŒ– V å€¼ã€‚

**æ€§èƒ½å¯¹æ¯”åˆ†æ**ï¼ˆåŸºäºç”¨æˆ·æµ‹è¯•æ•°æ®ï¼‰ï¼š

| é…ç½® | `ggml_compute_forward_flash_attn_ext` | åé‡åŒ–å¼€é”€ | æ€»æ—¶é—´ |
|------|--------------------------------------|-----------|--------|
| K: F16, V: F16 | 340ms | 0ms | 340ms |
| K: Q8_0, V: Q8_0 | 424ms | 127ms (`dequantize_row_q8_0`) | 424ms |

**æ ¹å› åˆ†æ**ï¼š

æŸ¥çœ‹ `ggml/src/ggml-cpu/ops.cpp` çš„ flash attention å®ç°ï¼š

```cpp
// ggml_compute_forward_flash_attn_ext_f16_one_chunk
ggml_to_float_t const v_to_float = ggml_get_type_traits(v->type)->to_float;

// åœ¨ attention å¾ªç¯ä¸­
if (v->type == GGML_TYPE_F16) {
    // å¿«é€Ÿè·¯å¾„ï¼šç›´æ¥ä½¿ç”¨ F16 æ“ä½œ
    ggml_vec_mad_f16(DV, VKQ16, (const ggml_fp16_t *) v_data, vs);
} else {
    // æ…¢é€Ÿè·¯å¾„ï¼šæ¯æ¬¡è¿­ä»£éƒ½éœ€è¦åé‡åŒ–
    v_to_float(v_data, V32, DV);  // <- è¿™é‡Œè°ƒç”¨ dequantize_row_q8_0
    ggml_vec_mad_f32(DV, VKQ32, V32, vs);
}
```

**æ•°æ®æµè¿‡ç¨‹**ï¼š
1. è®¡ç®— KÃ—Q å¾—åˆ° attention scoresï¼ˆK é‡åŒ–å¯ç”¨ `vec_dot_q8_0_q8_0` å¿«é€Ÿè®¡ç®—ï¼‰
2. å¯¹äº Vï¼šæ¯ä¸ª attention step éƒ½éœ€è¦å°† V ä» Q8_0 åé‡åŒ–ä¸º F32
3. åé‡åŒ–åœ¨ **çƒ­å¾ªç¯** å†…æ‰§è¡Œï¼Œå¯¼è‡´æ˜¾è‘—å¼€é”€

**ä¼˜åŒ–å»ºè®®**ï¼š

1. **æ¨èé…ç½®**ï¼šK ä½¿ç”¨é‡åŒ–ï¼ˆèŠ‚çœå†…å­˜+è®¡ç®—ï¼‰ï¼ŒV ä¿æŒ F16ï¼ˆé¿å…åé‡åŒ–å¼€é”€ï¼‰
   ```bash
   ./bin/whisper-cli -m model.bin -f audio.wav --kv-type-k q8_0 --kv-type-v f16
   ```

2. **ggml å±‚é¢ä¼˜åŒ–**ï¼ˆéœ€è¦ä¿®æ”¹ ggml åº“ï¼‰ï¼š
   - å®ç° `ggml_vec_mad_q8_0` ç­‰ç›´æ¥æ“ä½œé‡åŒ–æ•°æ®çš„å‡½æ•°
   - å‚è€ƒ [ik_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp) çš„é‡åŒ– attention å®ç°

3. **é¢„åé‡åŒ–ç­–ç•¥**ï¼šåœ¨ attention è®¡ç®—å‰ä¸€æ¬¡æ€§åé‡åŒ–æ•´å±‚ Vï¼Œè€Œéé€è¡Œåé‡åŒ–

---

## ç¬¬å››é˜¶æ®µï¼šæ˜“äºå®ç°çš„åˆ›æ–°ä¼˜åŒ–æ–¹æ¡ˆ (Practical Innovations)

åŸºäºå½“å‰ whisper.cpp çš„ KV Cache å®ç°ï¼Œä»¥ä¸‹æ˜¯å‡ ç§**å·¥ç¨‹å¯è¡Œæ€§é«˜ã€å…·æœ‰åˆ›æ–°æ€§**çš„ä¼˜åŒ–æ–¹æ¡ˆï¼š

### 4.1 æ–¹æ¡ˆä¸€ï¼šåŠ¨æ€ KV Cache å¤§å°è°ƒæ•´ï¼ˆæ¨è â­â­â­ï¼‰

**åˆ›æ–°ç‚¹**ï¼šæ ¹æ®å®é™…éŸ³é¢‘é•¿åº¦åŠ¨æ€è°ƒæ•´ KV Cache å¤§å°ï¼Œé¿å…å›ºå®šåˆ†é… 448 tokens çš„æµªè´¹ã€‚

**å®ç°éš¾åº¦**ï¼šä½

**åŸç†**ï¼šå½“å‰å®ç°é¢„åˆ†é… `n_ctx = 448` å¤§å°çš„ KV Cacheï¼Œä½†å¤§å¤šæ•°éŸ³é¢‘ç‰‡æ®µå®é™…ä½¿ç”¨çš„ token æ•°è¿œå°äºæ­¤ã€‚

**å®ç°ä»£ç **ï¼š
```cpp
// åœ¨ whisper_init_state ä¸­æ ¹æ®é¢„ä¼°éŸ³é¢‘é•¿åº¦è°ƒæ•´
static int estimate_kv_cache_size(float audio_duration_sec) {
    // Whisper æ¯ 30 ç§’éŸ³é¢‘çº¦äº§ç”Ÿ ~200-300 tokens
    // ä¿ç•™ 20% ä½™é‡
    int estimated_tokens = (int)(audio_duration_sec * 10.0f * 1.2f);
    return std::min(estimated_tokens, 448);  // ä¸Šé™ 448
}

// ä¿®æ”¹ whisper_kv_cache_init è°ƒç”¨
int dynamic_ctx = estimate_kv_cache_size(audio_duration);
whisper_kv_cache_init(state->kv_self, backend, itype, 
    n_text_state, n_text_layer, dynamic_ctx);
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- çŸ­éŸ³é¢‘ï¼ˆ<10ç§’ï¼‰å†…å­˜èŠ‚çœ ~60-70%
- æ— ç²¾åº¦æŸå¤±
- å®Œå…¨å‘åå…¼å®¹

### 4.2 æ–¹æ¡ˆäºŒï¼šKV Cache æƒ°æ€§åˆ†é…ï¼ˆæ¨è â­â­â­ï¼‰

**åˆ›æ–°ç‚¹**ï¼šå»¶è¿Ÿ KV Cache çš„å®é™…å†…å­˜åˆ†é…ï¼Œç›´åˆ°çœŸæ­£éœ€è¦æ—¶æ‰åˆ†é…ã€‚

**å®ç°éš¾åº¦**ï¼šä½

**åŸç†**ï¼šå½“å‰ `whisper_init_state` åœ¨åˆå§‹åŒ–æ—¶å°±åˆ†é…å…¨éƒ¨ KV Cache å†…å­˜ã€‚æ”¹ä¸ºæŒ‰éœ€åˆ†é…å¯ä»¥ä¼˜åŒ–å¤šæ¨¡å‹åœºæ™¯ã€‚

**å®ç°ä»£ç **ï¼š
```cpp
struct whisper_kv_cache {
    // æ–°å¢æ ‡å¿—
    bool allocated = false;
    
    // ä¿å­˜åˆå§‹åŒ–å‚æ•°ï¼Œå»¶è¿Ÿåˆ†é…
    ggml_backend_t pending_backend = nullptr;
    ggml_type pending_wtype;
    int64_t pending_n_state;
    int64_t pending_n_layer;
    int pending_n_ctx;
};

// æƒ°æ€§åˆ†é…å‡½æ•°
static bool whisper_kv_cache_ensure_allocated(whisper_kv_cache & cache) {
    if (cache.allocated) return true;
    
    bool ok = whisper_kv_cache_init_internal(
        cache, cache.pending_backend, cache.pending_wtype,
        cache.pending_n_state, cache.pending_n_layer, cache.pending_n_ctx);
    
    cache.allocated = ok;
    return ok;
}
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- åŠ é€Ÿæ¨¡å‹åŠ è½½ï¼ˆå»¶è¿Ÿåˆ†é…å¤§å†…å­˜å—ï¼‰
- æ”¯æŒæŒ‰éœ€æ‰©å®¹

### 4.3 æ–¹æ¡ˆä¸‰ï¼šCross-Attention KV Cache å¤ç”¨ï¼ˆæ¨è â­â­ï¼‰

**åˆ›æ–°ç‚¹**ï¼šå¯¹äºç›¸åŒçš„ Encoder è¾“å‡ºï¼Œå¤ç”¨ Cross-Attention çš„ KV Cacheã€‚

**å®ç°éš¾åº¦**ï¼šä¸­

**åŸç†**ï¼šWhisper çš„ Cross-Attention K/V æ¥è‡ª Encoder è¾“å‡ºï¼Œå¯¹åŒä¸€éŸ³é¢‘çš„å¤šæ¬¡è§£ç ï¼ˆå¦‚ beam searchï¼‰å¯ä»¥å…±äº«ã€‚

**å®ç°ä»£ç **ï¼š
```cpp
struct whisper_state {
    // æ–°å¢ï¼šCross KV ç¼“å­˜çš„å¼•ç”¨è®¡æ•°
    int kv_cross_ref_count = 0;
    bool kv_cross_valid = false;
    
    // ç¼–ç å™¨è¾“å‡ºçš„ hashï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦å¯å¤ç”¨
    uint64_t encoder_output_hash = 0;
};

// æ£€æŸ¥æ˜¯å¦å¯å¤ç”¨
static bool can_reuse_cross_kv(whisper_state * state, uint64_t new_hash) {
    return state->kv_cross_valid && state->encoder_output_hash == new_hash;
}

// åœ¨ whisper_encode åæ ‡è®°æœ‰æ•ˆ
state->encoder_output_hash = compute_hash(encoder_output);
state->kv_cross_valid = true;
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- Beam Search åœºæ™¯ä¸‹å‡å°‘ ~50% çš„ Cross-KV å†…å­˜
- å¤šæ¬¡è§£ç åŒä¸€éŸ³é¢‘æ—¶æ˜¾è‘—åŠ é€Ÿ

### 4.4 æ–¹æ¡ˆå››ï¼šKV Cache å†…å­˜æ± ï¼ˆæ¨è â­â­ï¼‰

**åˆ›æ–°ç‚¹**ï¼šä½¿ç”¨å†…å­˜æ± ç®¡ç† KV Cacheï¼Œå‡å°‘é¢‘ç¹åˆ†é…/é‡Šæ”¾çš„å¼€é”€ã€‚

**å®ç°éš¾åº¦**ï¼šä¸­

**åŸç†**ï¼šä¸ºå¤šä¸ªæ¨ç†è¯·æ±‚å…±äº«ä¸€ä¸ª KV Cache å†…å­˜æ± ï¼Œé€šè¿‡æ§½ä½ç®¡ç†å®ç°é«˜æ•ˆå¤ç”¨ã€‚

**å®ç°ä»£ç **ï¼š
```cpp
struct whisper_kv_pool {
    std::vector<whisper_kv_cache> pool;
    std::vector<bool> in_use;
    std::mutex mtx;
    
    whisper_kv_cache * acquire() {
        std::lock_guard<std::mutex> lock(mtx);
        for (size_t i = 0; i < pool.size(); i++) {
            if (!in_use[i]) {
                in_use[i] = true;
                whisper_kv_cache_clear(pool[i]);
                return &pool[i];
            }
        }
        // æ‰©å®¹é€»è¾‘...
        return nullptr;
    }
    
    void release(whisper_kv_cache * cache) {
        std::lock_guard<std::mutex> lock(mtx);
        for (size_t i = 0; i < pool.size(); i++) {
            if (&pool[i] == cache) {
                in_use[i] = false;
                return;
            }
        }
    }
};
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- æœåŠ¡ç«¯åœºæ™¯ååé‡æå‡ 20-30%
- å‡å°‘å†…å­˜ç¢ç‰‡

### 4.5 æ–¹æ¡ˆäº”ï¼šé€‰æ‹©æ€§ KV Cache æ›´æ–°ï¼ˆæ¨è â­â­â­ï¼‰

**åˆ›æ–°ç‚¹**ï¼šä»…æ›´æ–°å˜åŒ–çš„ KV Cache ä½ç½®ï¼Œè€Œéæ•´ä½“é‡å†™ã€‚

**å®ç°éš¾åº¦**ï¼šä½

**åŸç†**ï¼šå½“å‰ `ggml_cpy` ä¼šå¤åˆ¶æ•´ä¸ª K/V å¼ é‡ã€‚å¯¹äºå¢é‡è§£ç åœºæ™¯ï¼Œåªéœ€æ›´æ–°æ–°å¢çš„ token ä½ç½®ã€‚

**å®ç°ä»£ç **ï¼š
```cpp
// åœ¨ whisper_build_graph_decoder ä¸­ä¼˜åŒ–
if (n_tokens == 1 && kv_head > 0) {
    // å¢é‡æ¨¡å¼ï¼šåªæ›´æ–°ä¸€ä¸ªä½ç½®
    struct ggml_tensor * k_slice = ggml_view_1d(ctx0, kv_self.k, 
        n_state, ggml_element_size(kv_self.k) * n_state * (il*n_ctx + kv_head));
    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Kcur, k_slice));
} else {
    // æ‰¹é‡æ¨¡å¼ï¼šç°æœ‰é€»è¾‘
    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Kcur, k));
}
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- å¢é‡è§£ç æ—¶å†…å­˜å¸¦å®½å‡å°‘ ~80%
- å¯¹é•¿åºåˆ—åœºæ™¯åŠ é€Ÿæ˜æ˜¾

### 4.6 å®ç°ä¼˜å…ˆçº§å»ºè®®

| æ–¹æ¡ˆ | åˆ›æ–°æ€§ | å®ç°éš¾åº¦ | é¢„æœŸæ”¶ç›Š | æ¨èä¼˜å…ˆçº§ |
|------|--------|----------|----------|------------|
| åŠ¨æ€å¤§å°è°ƒæ•´ | â˜…â˜…â˜† | ä½ | å†…å­˜ -60% | ğŸ¥‡ 1 |
| é€‰æ‹©æ€§æ›´æ–° | â˜…â˜…â˜… | ä½ | é€Ÿåº¦ +20% | ğŸ¥ˆ 2 |
| æƒ°æ€§åˆ†é… | â˜…â˜…â˜† | ä½ | åŠ è½½ +30% | ğŸ¥‰ 3 |
| Cross-KV å¤ç”¨ | â˜…â˜…â˜… | ä¸­ | å†…å­˜ -50% | 4 |
| å†…å­˜æ±  | â˜…â˜…â˜† | ä¸­ | åå +20% | 5 |

### 4.7 è®ºæ–‡åˆ›æ–°ç‚¹æç‚¼

å¯¹äºç¡•å£«è®ºæ–‡ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ä»¥ä¸‹åˆ›æ–°è§’åº¦ï¼š

1. **é¢å‘ç«¯ä¾§è®¾å¤‡çš„åŠ¨æ€å†…å­˜ç®¡ç†**
   - æ ¹æ®éŸ³é¢‘ç‰¹å¾åŠ¨æ€è°ƒæ•´ KV Cache å¤§å°
   - æå‡º"Audio-Aware KV Cache Sizing"ç®—æ³•

2. **å¢é‡å¼ KV Cache æ›´æ–°ç­–ç•¥**
   - åˆ©ç”¨ Whisper è‡ªå›å½’è§£ç çš„ç‰¹ç‚¹
   - å®ç°"Delta KV Update"æœºåˆ¶å‡å°‘å†…å­˜å¸¦å®½

3. **è·¨è§£ç å™¨ KV Cache å…±äº«**
   - åœ¨ Beam Search åœºæ™¯ä¸‹å…±äº« Cross-Attention KV
   - æå‡º"Cross-Decoder KV Sharing"æ¶æ„

è¿™äº›æ–¹æ¡ˆçš„å…±åŒç‰¹ç‚¹ï¼š
- ä¸ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œå…¼å®¹æ‰€æœ‰ Whisper æ¨¡å‹
- æ— ç²¾åº¦æŸå¤±ï¼ˆæˆ–å¯å¿½ç•¥ï¼‰
- å®ç°ä»£ç é‡å°ï¼ˆ100-300 è¡Œï¼‰
- å¯ç‹¬ç«‹éªŒè¯å’Œå‘è¡¨

---

## ç»“è®ºä¸å±•æœ›

æœ¬ç ”ç©¶ç³»ç»Ÿåˆ†æäº† `whisper.cpp` ä¸­ KV Cache çš„å®ç°æœºåˆ¶å’Œç†è®ºç“¶é¢ˆï¼Œæå‡ºäº†åŸºäº Q8_0 é‡åŒ–çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚

**å½“å‰çŠ¶æ€**ï¼š
- ç†è®ºåˆ†æå®Œæˆï¼Œç¡®è®¤ KV Cache é‡åŒ–å¯å¸¦æ¥ ~50% çš„å†…å­˜èŠ‚çœ
- å®ç°è¿‡ç¨‹ä¸­å‘ç° ggml å—é‡åŒ–ç±»å‹ä¸ç°æœ‰è§†å›¾æœºåˆ¶å­˜åœ¨å…¼å®¹æ€§é—®é¢˜
- éœ€è¦é‡æ„å¼ é‡åˆ›å»ºå’Œè§†å›¾è®¡ç®—é€»è¾‘ä»¥æ”¯æŒé‡åŒ–ç±»å‹

**å®ç°è·¯çº¿å›¾**ï¼š
1. **çŸ­æœŸ**ï¼šä¿®æ”¹ `whisper_build_graph_decoder` ä¸­çš„è§†å›¾åç§»è®¡ç®—ï¼Œä½¿ç”¨ `ggml_row_size()` 
2. **ä¸­æœŸ**ï¼šå®ç° K/V åˆ†ç¦»ç²¾åº¦é…ç½®ï¼Œå…è®¸ K ä½¿ç”¨ FP16ã€V ä½¿ç”¨ Q8_0
3. **é•¿æœŸ**ï¼šå®ç°è‡ªé€‚åº”é‡åŒ–ç­–ç•¥ï¼Œæ ¹æ®å±‚çº§å’Œæ—¶åºåŠ¨æ€é€‰æ‹©ç²¾åº¦

**åç»­ç ”ç©¶æ–¹å‘**ï¼š
- æ¢ç´¢æ›´æ¿€è¿›çš„ 4-bit (Q4_0) é‡åŒ–æ–¹æ¡ˆ
- ç»“åˆæ»‘åŠ¨çª—å£æ³¨æ„åŠ›è¿›ä¸€æ­¥ä¼˜åŒ–é•¿åºåˆ—æ€§èƒ½
- å¼€å‘è‡ªé€‚åº”é‡åŒ–ç­–ç•¥ï¼ˆæ ¹æ®æ•°å€¼åˆ†å¸ƒåŠ¨æ€é€‰æ‹©ç²¾åº¦ï¼‰
- å®ç°æ··åˆç²¾åº¦ç­–ç•¥ï¼šK/V åˆ†ç¦»ã€å±‚çº§å·®å¼‚åŒ–ã€æ—¶é—´è¡°å‡

---

## å‚è€ƒæºç ä½ç½®

| åŠŸèƒ½ | æ–‡ä»¶ | å‡½æ•°/ç»“æ„ä½“ |
|------|------|-------------|
| KV Cache å®šä¹‰ | src/whisper.cpp | `whisper_kv_cache`, `whisper_kv_cell` |
| KV Cache åˆå§‹åŒ– | src/whisper.cpp | `whisper_kv_cache_init` |
| Decoder å›¾æ„å»º | src/whisper.cpp | `whisper_build_graph_decoder` |
| KV Cache æ“ä½œ | src/whisper.cpp | `whisper_kv_cache_find_slot`, `whisper_kv_cache_clear` |
| Context å‚æ•° | include/whisper.h | `whisper_context_params` |
| ggml é‡åŒ–ç±»å‹ | ggml/include/ggml.h | `GGML_TYPE_Q8_0` |
